{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Plant_recognition_pt_no_sep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPF4DpO2kYwE9sBJYdTmX5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KubaSiwiec/plant_recognition/blob/main/Plant_recognition_pt_no_sep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZCSjt3ZfINb"
      },
      "source": [
        "# How to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmDvDb5WvLWF"
      },
      "source": [
        "To make code below work properly:\n",
        " \n",
        "\n",
        "1.   Download leafsnap dataset from [Leafsnap website](http://leafsnap.com/dataset/),\n",
        "2.   place leafsnap dataset of your Google Drive at location: /content/gdrive/MyDrive/leafsnap_data,\n",
        "\n",
        "1.   use the \n",
        "[data processing and orginizing script](https://colab.research.google.com/drive/1S803Ho4BLyO6zfwxHRzw27u9_ucFNrKY?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-W3Kj06fU7J"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MPy4MmR0ndY"
      },
      "source": [
        "Mount Google drive to Colab to access data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKSY5SlPPcqw",
        "outputId": "6abb319f-6dde-4f71-b64d-03ac20b7118b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN40I-u4fZk0"
      },
      "source": [
        "# Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggdFzRnlF4po"
      },
      "source": [
        "from __future__ import print_function, division\n",
        " \n",
        "from IPython.display import clear_output, display\n",
        " \n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "from skimage.feature import local_binary_pattern, hog, canny, corner_fast\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# import models\n",
        "from torchvision.models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2\n",
        "from torchvision.models.vgg  import vgg16\n",
        "from torchvision.models.inception import inception_v3\n",
        "from torchvision.models.googlenet import googlenet\n",
        "from torchvision.models.densenet import densenet121\n",
        "from torchvision.models.alexnet import alexnet\n",
        "from torchvision.models.mobilenetv3 import mobilenet_v3_small, mobilenet_v3_large\n",
        "from torchvision.models.squeezenet import squeezenet1_1\n",
        "\n",
        "\n",
        "from typing import Type, Any, Callable, Union, List, Optional, Dict, cast, Tuple, OrderedDict\n",
        "\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from datetime import date"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VZnBYUffhVg"
      },
      "source": [
        "# Define configuration class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KITxLQ7M0gtF"
      },
      "source": [
        "class Cfg(object):\n",
        "    '''\n",
        "    This is a class consisting of configuration for the future run\n",
        "    '''\n",
        "\n",
        "    def __init__(self, model_type, data_transform, learning_rate = 0.02, number_of_epochs = 50, scheduler_step_size = 20, criterion = torch.nn.CrossEntropyLoss(), lbp_radius = 3, save_model = False):\n",
        "        self.model_type = model_type\n",
        "        self.data_transform = data_transform\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_of_epochs = number_of_epochs\n",
        "        self.scheduler_step_size = scheduler_step_size\n",
        "        self.criterion = criterion\n",
        "        self.lbp_radius = lbp_radius\n",
        "        self.save_model = save_model\n",
        "        \n",
        "        \n",
        "\n",
        "    def set_optimizer(self, model_ft):\n",
        "        self.optimizer  = optim.SGD(model_ft.parameters(), lr=self.learning_rate, momentum=0.9)\n",
        "    def set_scheduler(self, optimizer_ft):\n",
        "        self.scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=self.scheduler_step_size, gamma=0.1)\n",
        "\n",
        "    def set_model_type(self, model_type):\n",
        "        self.model_type = model_type\n",
        "    def set_data_transform(self, data_transform):\n",
        "        self.data_transform = data_transform\n",
        "    def set_learning_rate(self, learning_rate):\n",
        "        self.learning_rate = learning_rate\n",
        "    def set_scheduler_step_size(self, scheduler_step_size):\n",
        "        self.scheduler_step_size = scheduler_step_size\n",
        "    \n",
        "    \n",
        "    def __repr__(self):\n",
        "        d_t_repr = (self.data_transform + '_rad' +  str(self.lbp_radius)) if self.data_transform in ['lbp', 'lbp_3'] else self.data_transform\n",
        "        return \"{}\\t{}\\t{}\\t{}\\t{}\".format(self.model_type, d_t_repr, self.learning_rate, self.number_of_epochs, self.scheduler_step_size)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ScbXRSUfwri"
      },
      "source": [
        "Set config before running rest of the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpyK65zEs_6s"
      },
      "source": [
        "cfg = Cfg('resnet18_1chan', 'lbp', 0.02, 1, criterion = torch.nn.CrossEntropyLoss())"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiNTpxtPf2wp"
      },
      "source": [
        "Print resources information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP5uAkqnHVC0",
        "outputId": "89c18f10-4a8c-410b-9724-3a43f95b625f"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul  5 12:07:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    30W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrKAO-RGf-y3"
      },
      "source": [
        "# Feature extraction functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKFGel74CoG"
      },
      "source": [
        "def local_binary_pattern_3channels(img, radius, n_points):\n",
        "    r, g, b = img.split()\n",
        "    lbp_r = local_binary_pattern(r, radius, n_points)\n",
        "    lbp_g = local_binary_pattern(g, radius, n_points)\n",
        "    lbp_b = local_binary_pattern(b, radius, n_points)\n",
        "    return (np.dstack((lbp_r,lbp_g,lbp_b)) / np.amax(np.dstack((lbp_r,lbp_g,lbp_b))) * 255).astype(np.uint8)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F46kAuhOKXGI"
      },
      "source": [
        "def canny_3channels(img):\n",
        "    r, g, b = img.split()\n",
        "    lbp_r = canny(np.asarray(r))\n",
        "    lbp_g = canny(np.asarray(g))\n",
        "    lbp_b = canny(np.asarray(b))\n",
        "    return (np.dstack((lbp_r,lbp_g,lbp_b)) / np.amax(np.dstack((lbp_r,lbp_g,lbp_b))) * 255).astype(np.uint8)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "6A9G4hzsMhP1",
        "outputId": "e677b3d9-f7f7-444b-f0ae-c844fa54c9b5"
      },
      "source": [
        "im = Image.open('/content/gdrive/MyDrive/leafsnap_data/leafsnap-dataset-resized-no-sep/acer_ginnala/13291762510376.jpg')\n",
        "plt.imshow(im)\n",
        "plt.imshow(Image.fromarray(canny_3channels(im)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe4f684a210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szNj8VWU5c3e"
      },
      "source": [
        "# w podobny sposób trzeba pisać nowe transformacje\n",
        "class Local_binary_pattern(object):\n",
        "      def __init__(self, radius):\n",
        "          assert isinstance(radius, int)\n",
        "          self.radius = radius\n",
        "          self.n_points = radius * 8\n",
        "\n",
        "      def __call__(self, img):\n",
        "          \"\"\"\n",
        "          Args:\n",
        "              img (PIL Image): Image to be scaled.\n",
        "          Returns:\n",
        "              PIL Image: Rescaled image.\n",
        "          \"\"\"\n",
        "          return Image.fromarray(local_binary_pattern(img, self.radius, self.n_points))\n",
        " \n",
        "      def __repr__(self):\n",
        "          return self.__class__.__name__ + '()'\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xemJEflLWXJn"
      },
      "source": [
        "class Local_binary_pattern3(object):\n",
        "      def __init__(self, radius):\n",
        "          assert isinstance(radius, int)\n",
        "          self.radius = radius\n",
        "          self.n_points = radius * 8\n",
        "\n",
        "      def __call__(self, img):\n",
        "          \"\"\"\n",
        "          Args:\n",
        "              img (PIL Image): Image to be scaled.\n",
        "          Returns:\n",
        "              PIL Image: Rescaled image.\n",
        "          \"\"\"\n",
        "          return Image.fromarray(local_binary_pattern_3channels(img, self.radius, self.n_points))\n",
        " \n",
        "      def __repr__(self):\n",
        "          return self.__class__.__name__ + '()'"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NefQet7Kgnk"
      },
      "source": [
        "class Canny3(object):\n",
        "\n",
        "      def __call__(self, img):\n",
        "          \"\"\"\n",
        "          Args:\n",
        "              img (PIL Image): Image to be scaled.\n",
        "          Returns:\n",
        "              PIL Image: Rescaled image.\n",
        "          \"\"\"\n",
        "          return Image.fromarray(canny_3channels(img))\n",
        " \n",
        "      def __repr__(self):\n",
        "          return self.__class__.__name__ + '()'"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpsyNzVNYhxY"
      },
      "source": [
        "# w podobny sposób trzeba pisać nowe transformacje\n",
        "class TrippleGrayscale(object):\n",
        "\n",
        "      def __call__(self, img):\n",
        "          \"\"\"\n",
        "          Args:\n",
        "              img (PIL Image): Image to be scaled.\n",
        "          Returns:\n",
        "              PIL Image: Rescaled image.\n",
        "          \"\"\"\n",
        "\n",
        "          \n",
        "\n",
        "          return Image.fromarray(local_binary_pattern(img, self.radius, self.n_points))\n",
        " \n",
        "      def __repr__(self):\n",
        "          return self.__class__.__name__ + '()'\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2gxcRc6segs"
      },
      "source": [
        "class Hog(object):\n",
        "      def __call__(self, img):\n",
        "          \"\"\"\n",
        "          Args:\n",
        "              img (PIL Image): Image to be scaled.\n",
        "          Returns:\n",
        "              PIL Image: Rescaled image.\n",
        "          \"\"\"\n",
        "          hog_descriptor = hog(img, visualize = False)\n",
        "          return hog_descriptor\n",
        " \n",
        "      def __repr__(self):\n",
        "          return self.__class__.__name__ + '()'"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKxtQWOG9xkn"
      },
      "source": [
        "PIL images are not writable, thus to prevent warnings, convertion to numpy before converting to torch.Tensor is necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yYcQDmxytfg"
      },
      "source": [
        "class ToNumpy(object):\n",
        "    def __call__(self, sample):\n",
        "        return np.array(sample)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvv333P0ANOh"
      },
      "source": [
        "# Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB4dC8Ya3WaX"
      },
      "source": [
        "# transforms\n",
        "data_transform = {\n",
        "    'original': transforms.Compose([\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))        \n",
        "    ]),\n",
        "    'grayscale': transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    'lbp': transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        Local_binary_pattern(cfg.lbp_radius),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'grayscale3': transforms.Compose([\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor()\n",
        "    ]),\n",
        "    'lbp3': transforms.Compose([\n",
        "        Local_binary_pattern3(cfg.lbp_radius),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n",
        "    ]),\n",
        "    'canny3': transforms.Compose([\n",
        "        Canny3(),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) \n",
        "    ]),\n",
        "    # there is necessity to extract histograms later\n",
        "    'hog': transforms.Compose([\n",
        "        # transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ToNumpy(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        lambda x: np.rollaxis(x.numpy(), 0, 3)\n",
        "    ])\n",
        "}\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hEoEvRWgKxX"
      },
      "source": [
        "# Define data loaders for lazy image loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHF8L3gmP6Jf"
      },
      "source": [
        "def get_data_loaders(cfg):\n",
        "    data = {x: datasets.ImageFolder('/content/gdrive/MyDrive/leafsnap_data/{}'.format(x), transform = data_transform[cfg.data_transform]) for x in ['leafsnap-dataset-resized-no-sep']}\n",
        "    data_length = len(data['leafsnap-dataset-resized-no-sep'])\n",
        "    train_size = int(data_length * 0.7)\n",
        "    val_size = int(data_length * 0.2)\n",
        "    test_size = data_length - train_size - val_size\n",
        "    plant_dataset = {}\n",
        "    plant_dataset['train'], plant_dataset['validation'], plant_dataset['test'] = torch.utils.data.random_split(data['leafsnap-dataset-resized-no-sep'], [train_size, val_size, test_size])\n",
        "    dataloaders = {x: DataLoader(plant_dataset[x], batch_size = 32, num_workers = 4, shuffle = True, drop_last = True) for x in ['train', 'validation', 'test']}\n",
        "    dataset_size = {x: len(plant_dataset[x]) for x in ['train', 'validation', 'test']}\n",
        "    print('Training dataset size: {}'.format(dataset_size['train']))\n",
        "    print('Validation dataset size: {}'.format(dataset_size['validation']))\n",
        "    print('Test dataset size: {}'.format(dataset_size['test']))\n",
        "    return dataloaders, dataset_size, data"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cO1v29XgTYd"
      },
      "source": [
        "# Imshow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tag750qAoLqT"
      },
      "source": [
        "def imshow(image):\n",
        "  if isinstance(image, torch.Tensor):\n",
        "    image = image.numpy().transpose((1,2,0))\n",
        "  else:\n",
        "    image = np.array(image).transpose((1,2,0))\n",
        "  # unnormalize\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  image = std * image + mean\n",
        "  image = np.clip(image, 0, 1)\n",
        "\n",
        "  # plot\n",
        "  fig, ax = plt.subplots(1,1, figsize=(15,15))\n",
        "  plt.imshow(image)\n",
        "  ax.axis('off')\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEsXiqaegY03"
      },
      "source": [
        "\n",
        "\n",
        "# Set device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vBAQ0NxLwYG"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNlLpUC0gf9b"
      },
      "source": [
        "# F1-score calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amCnlStsfAo2"
      },
      "source": [
        "def f1score(y_true, y_pred):\n",
        "      tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
        "      tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "      fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
        "      fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "\n",
        "      precision = tp / (tp + fp + self.epsilon)\n",
        "      recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "      f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
        "      f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
        "      print(tp, tn, fp, fn, precision, recall)\n",
        "      # return 1 - f1.mean()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2MkADvXmOWw"
      },
      "source": [
        "def get_weighted_f1(f1_arr, labels):\n",
        "    labels_count = collections.Counter(labels)\n",
        "    ordered_labels_count = collections.OrderedDict(sorted(labels_count.items()))\n",
        "\n",
        "    all_images_count = sum(ordered_labels_count.values())\n",
        "    print(all_images_count)\n",
        "    f1_weighted = 0\n",
        "    for idx, (key, value) in enumerate(ordered_labels_count.items()):\n",
        "        f1_weighted += value * f1_arr[idx]\n",
        "    f1_weighted /= all_images_count\n",
        "    print(f1_weighted)\n",
        "    return f1_weighted"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n49JO-cvglP7"
      },
      "source": [
        "# Display functions for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkNDxHl3rU5y"
      },
      "source": [
        "def progress_bar(samples_already_processed, total_samples):\n",
        "    nb_spaces = int(((total_samples - samples_already_processed) / total_samples) * 70)\n",
        "    nb_eq_signs = int((samples_already_processed / total_samples) * 70)\n",
        "    bar = '||' + '=' * nb_eq_signs + '>' + ' ' * nb_spaces + '||    ' + str(samples_already_processed) + '/' + str(total_samples)\n",
        "    return bar"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RTRJ85YvrNa"
      },
      "source": [
        "def print_train_info(model):\n",
        "    print('Model: {}'.format(model))\n",
        "    print('\\n' * 10 + '-' * 10 + '\\t' + '-' * 10 + '\\t' + '-' * 10 + '\\n' + \n",
        "          '|' + ' ' * 10 + '\\t' + ' ' * 10 + '\\t' + ' ' * 10  + '|' + '\\n' +\n",
        "          '|' + ' ' * 10 + '\\tTRAINING\\t' + ' ' * 10  + '|' + '\\n' + \n",
        "          '|' + ' ' * 10 + '\\t' + ' ' * 10 + '\\t' + ' ' * 10  + '|' + '\\n' +\n",
        "          '-' * 10 + '\\t' + '-' * 10 + '\\t' + '-' * 10 + '\\n\\n\\n')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qaq8iwVgqcO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Hojvuhu23C"
      },
      "source": [
        "def train_model(model, cfg: Cfg, print_epoch_info: str = 'detailed'):\n",
        "    print_train_info(model)\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    epochs_info = 'First of {} epochs started:'.format(cfg.number_of_epochs)\n",
        "    epoch_progress_bar = ''\n",
        "    training_progress_bar = ''\n",
        "    epochs_disp = display(epochs_info, display_id = True)\n",
        "    print('Epochs:')\n",
        "    training_progress_display = display(training_progress_bar, display_id = True)\n",
        "    print('\\nImages processed:')\n",
        "    epoch_progress_disp = display(epochs_disp, display_id = True)\n",
        "    print('\\n\\n\\n\\n\\n')\n",
        "\n",
        "    train_acc = np.array([])\n",
        "    val_acc = np.array([])\n",
        "\n",
        "    for epoch in range(cfg.number_of_epochs):\n",
        "        training_progress_display.update(progress_bar(epoch + 1, cfg.number_of_epochs))\n",
        "        start_epoch_time = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'validation']:\n",
        "            if phase == 'train':\n",
        "                dataset = 'train'\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                dataset = 'validation'\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for batch_index, (inputs, labels) in enumerate(dataloaders[dataset]):\n",
        "                if cfg.data_transform == 'hog':\n",
        "                    inputs = hog(inputs, visualize = False)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                cfg.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = cfg.criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        cfg.optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                if print_epoch_info == 'detailed':\n",
        "                    epoch_time = time.time() - start_epoch_time\n",
        "                    epoch_progress_bar = progress_bar(batch_index * dataloaders[dataset].batch_size + 1, dataset_size[dataset])\n",
        "                    epoch_progress_disp.update(epoch_progress_bar)\n",
        "            if phase == 'train':\n",
        "                cfg.scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size[dataset]\n",
        "            epoch_acc = running_corrects.double() / dataset_size[dataset]\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc = np.append(train_acc, epoch_acc.cpu().numpy())\n",
        "            if phase == 'validation':\n",
        "                val_acc = np.append(val_acc, epoch_acc.cpu().numpy())\n",
        "            if print_epoch_info in ['detailed']:\n",
        "                epochs_info = 'Epoch {}, {}, Loss: {:.4f} Acc: {:.4f} Time: {}'.format(\n",
        "                    epoch, phase, epoch_loss, epoch_acc, epoch_time)\n",
        "                # clear_output(wait=False)\n",
        "            if print_epoch_info in ['general']:\n",
        "                epochs_info = 'Epoch {}, {} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                    epoch, phase, epoch_loss, epoch_acc)\n",
        "            epochs_disp.update(epochs_info)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'validation' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, time_elapsed, train_acc, val_acc"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L6ZbAl5gt7D"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5HXGeYw_pO9"
      },
      "source": [
        "def test_model(model):\n",
        "    start_time = time.time()\n",
        "\n",
        "    \n",
        "    dataset = 'test'\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    running_corrects5 = 0\n",
        "\n",
        "    y_real = np.array([])\n",
        "    y_pred = np.array([])\n",
        "\n",
        "    # Iterate over data.\n",
        "    for batch_index, (inputs, labels) in enumerate(dataloaders[dataset]):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            _, preds5 = torch.topk(outputs, 5, 1)\n",
        "        # statistics\n",
        "        # clear_output(wait=False)\n",
        "        y_real = np.append(y_real, labels.data.cpu().numpy())\n",
        "        y_pred = np.append(y_pred, preds.cpu().numpy())\n",
        "        \n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        for i, single_pred in enumerate(preds5):\n",
        "            running_corrects5 += 1 if (labels.data[i] in single_pred) else 0\n",
        "\n",
        "        print(\"{}/{} images used for testing.\".format(batch_index * dataloaders[dataset].batch_size + 1, dataset_size[dataset]))\n",
        "        # clear_output(wait=False)\n",
        "\n",
        "    test_acc = running_corrects.double() / dataset_size[dataset]\n",
        "    test_acc5 = float(running_corrects5 / dataset_size[dataset])\n",
        "\n",
        "    print('Test accuracy: {:.4f}\\n'.format(test_acc))\n",
        "    print('Top 5 test accuracy: {:.4f}\\n'.format(test_acc5))\n",
        "    f1 = f1_score(y_real, y_pred, average=None)\n",
        "    print(\"F score: {}\".format(f1))\n",
        "\n",
        "    return test_acc, f1, y_real, test_acc5"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw2BPDTngy6I"
      },
      "source": [
        "# Get results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k9SyIzOKU6J"
      },
      "source": [
        "def plot_training_acc(train_acc, val_acc, cfg: Cfg):\n",
        "    plots_folder = \"/content/gdrive/MyDrive/leafsnap_training_accuracy\"\n",
        "    Path(plots_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    plot_save_path = \"{}/{}_{}_{}_{}_{}.png\".format(plots_folder, cfg.model_type, cfg.data_transform, cfg.learning_rate, cfg.scheduler_step_size, date.today())\n",
        "\n",
        "    plt.figure(figsize=(12, 8), dpi=60)\n",
        "    plt.subplot(211)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Accuracy')\n",
        "    plt.plot(train_acc)\n",
        "    plt.hlines(train_acc.max(), 0, cfg.number_of_epochs - 1, colors=None, linestyles='dashed', label=str(train_acc.max()))\n",
        "    plt.text(0, train_acc.max(), round(train_acc.max(),3), ha='right', va='center')\n",
        "    plt.subplot(212)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Accuracy')   \n",
        "    plt.plot(val_acc)\n",
        "    plt.hlines(val_acc.max(), 0, cfg.number_of_epochs - 1, colors=None, linestyles='dashed', label=str(val_acc.max()))\n",
        "    plt.text(0, val_acc.max(), round(val_acc.max(),3), ha='right', va='center')\n",
        "\n",
        "    plt.savefig(plot_save_path)\n",
        "\n",
        "    "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EobUpWmg3bt"
      },
      "source": [
        "# NN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2IhCTFqGZnI"
      },
      "source": [
        "class LSTM_network():\n",
        "    def __init__(self):\n",
        "      pass"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaqSZhsQHbo6"
      },
      "source": [
        "Modify torchvision resnet architectures to make them able to process single channel images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpj9eWeU35JE"
      },
      "source": [
        "class ResNet_1chan(models.resnet.ResNet):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes = 187, width_per_group = 4, groups = 32):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet_1chan, self).__init__(block, layers)\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        \n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[models.resnet.BasicBlock, models.resnet.Bottleneck]],\n",
        "    layers: List[int],\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> ResNet_1chan:\n",
        "    model = ResNet_1chan(block, layers, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    return _resnet('resnet18', models.resnet.BasicBlock, [2, 2, 2, 2], progress, **kwargs)\n",
        "\n",
        "def resnet34_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    return _resnet('resnet34', models.resnet.BasicBlock, [3, 4, 6, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "def resnet50_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    return _resnet('resnet50', models.resnet.Bottleneck, [3, 4, 6, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet101_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    return _resnet('resnet101', models.resnet.Bottleneck, [3, 4, 23, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "def resnet152_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    return _resnet('resnet152', models.resnet.Bottleneck, [3, 8, 36, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "def resnext50_32x4d_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', models.resnet.Bottleneck, [3, 4, 6, 3],\n",
        "                   progress, **kwargs)\n",
        "\n",
        "\n",
        "def resnext101_32x8d_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', models.resnet.Bottleneck, [3, 4, 23, 3],\n",
        "                   progress, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet50_2_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', models.resnet.Bottleneck, [3, 4, 6, 3],\n",
        "                   progress, **kwargs)\n",
        "\n",
        "\n",
        "def wide_resnet101_2_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet_1chan:\n",
        "    kwargs['num_classes'] = len(class_names)\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', models.resnet.Bottleneck, [3, 4, 23, 3],\n",
        "                   progress, **kwargs)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1U5jyCzPaqF"
      },
      "source": [
        "VGG16 for 1 channel input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRy9nSDuMRel"
      },
      "source": [
        "from torchvision.models.vgg import VGG\n",
        "\n",
        "# cfgs for different vgg versions\n",
        "cfgs: Dict[str, List[Union[str, int]]] = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def make_layers_1chan(cfg: List[Union[str, int]], batch_norm: bool = False) -> nn.Sequential:\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = 1\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            v = cast(int, v)\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def _vgg_1chan(arch: str, vgg_cfg: str, batch_norm: bool, pretrained: bool, progress: bool, **kwargs: Any) -> VGG:\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "    model = VGG(make_layers_1chan(cfgs[vgg_cfg], batch_norm=batch_norm), **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg16_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n",
        "    return _vgg_1chan('vgg16_1chan', 'D', False, False, progress, **kwargs)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXjPLToVR08D"
      },
      "source": [
        "Customization for densenet to process one-channel input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQk-0c2xR0AX"
      },
      "source": [
        "class DenseNet_1chan(models.densenet.DenseNet):\n",
        "\n",
        "    def __init__(self,\n",
        "        growth_rate: int = 32,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 1000,\n",
        "        memory_efficient: bool = False\n",
        "    ) -> None:\n",
        "        super(DenseNet_1chan, self).__init__(growth_rate,\n",
        "        block_config,\n",
        "        num_init_features,\n",
        "        bn_size,\n",
        "        drop_rate,\n",
        "        num_classes,\n",
        "        memory_efficient)\n",
        "        \n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=7, stride=2,\n",
        "                                padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "def _densenet_1chan(\n",
        "    arch: str,\n",
        "    growth_rate: int,\n",
        "    block_config: Tuple[int, int, int, int],\n",
        "    num_init_features: int,\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> DenseNet_1chan:\n",
        "    model = DenseNet_1chan(growth_rate, block_config, num_init_features, **kwargs)\n",
        "    if pretrained:\n",
        "        _load_state_dict(model, model_urls[arch], progress)\n",
        "    return model\n",
        "\n",
        "def densenet121_1chan(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet_1chan:\n",
        "    return _densenet_1chan('densenet121_1chan', 32, (6, 12, 24, 16), 64, False, False,\n",
        "                     **kwargs)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36SfETJnYAa6"
      },
      "source": [
        "Redefine alexnet to process single channel images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS_W-rxgYIRF"
      },
      "source": [
        "from torchvision.models.alexnet import AlexNet\n",
        "class AlexNet_1chan(AlexNet):\n",
        "\n",
        "    def __init__(self, num_classes: int = 1000) -> None:\n",
        "        super(AlexNet_1chan, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "def alexnet_1chan(**kwargs: Any) -> AlexNet:\n",
        "    model = AlexNet_1chan(**kwargs)\n",
        "    return model"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFLsPJuoIUO"
      },
      "source": [
        "class SqueezeNet_1chan(models.squeezenet.SqueezeNet):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 1000\n",
        "    ) -> None:\n",
        "        super(SqueezeNet_1chan, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(64, 16, 64, 64),\n",
        "            Fire(128, 16, 64, 64),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(128, 32, 128, 128),\n",
        "            Fire(256, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(256, 48, 192, 192),\n",
        "            Fire(384, 48, 192, 192),\n",
        "            Fire(384, 64, 256, 256),\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "\n",
        "def _squeezenet_1chan(**kwargs: Any) -> SqueezeNet_1chan:\n",
        "    model = SqueezeNet_1chan(version, **kwargs)\n",
        "    return model\n",
        "\n",
        "def squeezenet1_1_1chan(**kwargs: Any) -> SqueezeNet_1chan:\n",
        "    return _squeezenet_1chan(**kwargs)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkeUhU-4ES86"
      },
      "source": [
        "Define model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ytRxPXzHsY"
      },
      "source": [
        "def get_model(cfg):\n",
        "    # Get model name from configuration and call function creating it\n",
        "    model_type = globals().get(cfg.model_type)\n",
        "    print(model_type)\n",
        "    model_ft = model_type(num_classes = len(class_names))\n",
        "\n",
        "    cfg.set_optimizer(model_ft)\n",
        "    cfg.set_scheduler(cfg.optimizer)\n",
        "\n",
        "    # model_ft = resnet18_1chan()\n",
        "    # num_ftrs = model_ft.fc.in_features\n",
        "    # Here the size of each output sample is set to len(class_names).\n",
        "    # model_ft.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
        "    return model_ft.to(device)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSQDMij2mPyG"
      },
      "source": [
        "Test model - resturn test accuracy and the array of F1-scores for each label and Y-real for purpose of computing weighted F1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vasY4GAPg_ss"
      },
      "source": [
        "# Visualize images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zgdYzDwbzcQ"
      },
      "source": [
        "def show_images(dataloaders):\n",
        "    images, labels = iter(dataloaders['train']).next()\n",
        "    out = torchvision.utils.make_grid(images, nrow = 8)\n",
        "    imshow(out)\n",
        "    print(labels)\n",
        "\n",
        "def print_histograms(dataloaders):\n",
        "    imgs, labels = iter(dataloaders['train']).next()\n",
        "    # imshow(imgs[0])\n",
        "    for img in imgs.numpy():\n",
        "        histogram, hog_img = hog(img, visualize = True, pixels_per_cell= (16, 16), cells_per_block= (2, 2))\n",
        "        print(len(histogram))\n",
        "        print(labels)\n",
        "        imshow(hog_img)\n",
        "\n",
        "def visualize(cfg: Cfg, dataloaders):\n",
        "    if cfg.data_transform == 'hog':\n",
        "        print_histograms(dataloaders)\n",
        "    else:\n",
        "        show_images(dataloaders)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbQkhXZhd-3h"
      },
      "source": [
        "def get_class_names(data):\n",
        "    class_names = data['leafsnap-dataset-resized-no-sep'].classes\n",
        "    print(class_names)\n",
        "    print(len(class_names))\n",
        "    return class_names"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HGJiinGm-8X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btbm9vtA-BNf"
      },
      "source": [
        "# Save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv7aEoEWrIob"
      },
      "source": [
        "def write_results(cfg, test_acc, test_acc5, f1_weighted, training_time):\n",
        "    with open(\"/content/gdrive/MyDrive/results_top5.tsv\", \"a+\") as results_file:\n",
        "        if os.stat(\"/content/gdrive/MyDrive/results_top5.tsv\").st_size == 0:\n",
        "            results_file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\"model_type\", \"data_transform\", \"learning_rate\", \"number_of_epochs\", \"scheduler_step_size\", \"training_time\", \"test_accuracy\", \"top_5_test_accuracy\", \"F1-score\"))\n",
        "        results_file.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(cfg, training_time, test_acc, test_acc5, f1_weighted))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFxesrx3f2fw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdbOAnFE-EbD"
      },
      "source": [
        "# Execution pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bnd2uvx6BJO"
      },
      "source": [
        "def run():\n",
        "    '''\n",
        "    This is the run() funtion. It is responsible for executing the whole program\n",
        "    '''\n",
        "    transforms = ['original', 'grayscale', 'lbp3', 'canny3', 'hog']\n",
        "    # transforms = ['lbp3']\n",
        "    # transforms = ['grayscale']\n",
        "    for transform in transforms:\n",
        "        cfg.set_data_transform(transform)\n",
        "        global dataloaders, dataset_size, class_names\n",
        "        dataloaders, dataset_size, data = get_data_loaders(cfg)\n",
        "        class_names = get_class_names(data)\n",
        "        visualize(cfg, dataloaders)\n",
        "        if cfg.data_transform == 'original':\n",
        "            models = ['vgg16', \n",
        "                      'resnet18', \n",
        "                      'resnet34', \n",
        "                      'resnet50', \n",
        "                      'resnet101', \n",
        "                      'resnet152', \n",
        "                      'resnext50_32x4d', \n",
        "                      'resnext101_32x8d', \n",
        "                      'wide_resnet50_2', \n",
        "                      'wide_resnet101_2',\n",
        "                      'inception_v3',\n",
        "                      'googlenet',\n",
        "                      'densenet121',\n",
        "                      'alexnet',\n",
        "                      'mobilenet_v3_small', \n",
        "                      'mobilenet_v3_large',\n",
        "                      'squeezenet1_1']\n",
        "            # models = [#'inception_v3',\n",
        "            #           # 'googlenet',\n",
        "            #           'densenet121',\n",
        "            #           'alexnet',\n",
        "            #           'mobilenet_v3_small', \n",
        "            #           'mobilenet_v3_large',\n",
        "            #           'squeezenet1_1']\n",
        "        else:\n",
        "            # models = ['resnet18_1chan', 'resnet34_1chan', 'resnet50_1chan', 'resnet101_1chan', 'resnet152_1chan', 'resnext50_32x4d_1chan', 'resnext101_32x8d_1chan', 'wide_resnet50_2_1chan', 'wide_resnet101_2_1chan', 'vgg16_1chan', 'densenet121_1chan']\n",
        "            models = [ 'vgg16', \n",
        "                      'resnet18', \n",
        "                      'resnet34', \n",
        "                      'resnet50', \n",
        "                      'resnet101', \n",
        "                      'resnet152', \n",
        "                      'resnext50_32x4d', \n",
        "                      'resnext101_32x8d', \n",
        "                      'wide_resnet50_2', \n",
        "                      'wide_resnet101_2',\n",
        "                      'densenet121',\n",
        "                      'alexnet',\n",
        "                      'mobilenet_v3_small', \n",
        "                      'mobilenet_v3_large',\n",
        "                      'squeezenet1_1']\n",
        "        \n",
        "        for nn_model in models:\n",
        "            print(\"Transform: {}, model: {}\".format(transform, nn_model))\n",
        "            cfg.set_model_type(nn_model)\n",
        "            model_ft = get_model(cfg)\n",
        "            model_ft, time_elapsed, train_acc, val_acc = train_model(model_ft, cfg, print_epoch_info = 'detailed')\n",
        "            plot_training_acc(train_acc, val_acc, cfg)\n",
        "            test_acc, f1_each_label, labels, test_acc5 = test_model(model_ft)\n",
        "            f1_weighted = get_weighted_f1(f1_each_label, labels)\n",
        "            write_results(cfg, test_acc, test_acc5, f1_weighted, time_elapsed)\n",
        "            print(\"Trained {} on {} pictures\".format(cfg.model_type, cfg.data_transform))\n",
        "            if cfg.save_model:\n",
        "                model_save_folder = '/content/gdrive/MyDrive/plant_recognition_models'\n",
        "                Path(model_save_folder).mkdir(parents=True, exist_ok=True)\n",
        "                model_save_path = model_save_folder + '/model_' + cfg.model_type + '_' + cfg.data_transform + '_' + str(date.today())\n",
        "                torch.save(model_ft.state_dict(), model_save_path)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D0lc-UWa9iNQ",
        "outputId": "27f05620-adc2-45e2-f11a-cb7afea873f9"
      },
      "source": [
        "# In the line below it is possible to choose the starting configuration \n",
        "cfg = Cfg('resnet18_1chan', 'grayscale', 0.02, 30, lbp_radius=3, save_model = True)\n",
        "run()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset size: 20148\n",
            "Validation dataset size: 5756\n",
            "Test dataset size: 2880\n",
            "['acer_campestre', 'acer_ginnala', 'acer_griseum', 'acer_negundo', 'acer_palmatum', 'acer_pensylvanicum', 'acer_platanoides', 'acer_pseudoplatanus', 'acer_rubrum', 'acer_saccharinum', 'acer_saccharum', 'aesculus_flava', 'aesculus_glabra', 'aesculus_hippocastamon', 'aesculus_pavi', 'ailanthus_altissima', 'albizia_julibrissin', 'amelanchier_arborea', 'amelanchier_canadensis', 'amelanchier_laevis', 'asimina_triloba', 'betula_alleghaniensis', 'betula_jacqemontii', 'betula_lenta', 'betula_nigra', 'betula_populifolia', 'broussonettia_papyrifera', 'carpinus_betulus', 'carpinus_caroliniana', 'carya_cordiformis', 'carya_glabra', 'carya_ovata', 'carya_tomentosa', 'castanea_dentata', 'catalpa_bignonioides', 'catalpa_speciosa', 'celtis_occidentalis', 'celtis_tenuifolia', 'cercidiphyllum_japonicum', 'cercis_canadensis', 'chamaecyparis_pisifera', 'chamaecyparis_thyoides', 'chionanthus_retusus', 'chionanthus_virginicus', 'cladrastis_lutea', 'cornus_florida', 'cornus_kousa', 'cornus_mas', 'corylus_colurna', 'crataegus_crus-galli', 'crataegus_laevigata', 'crataegus_phaenopyrum', 'crataegus_pruinosa', 'crataegus_viridis', 'cryptomeria_japonica', 'diospyros_virginiana', 'eucommia_ulmoides', 'evodia_daniellii', 'fagus_grandifolia', 'ficus_carica', 'fraxinus_americana', 'fraxinus_nigra', 'fraxinus_pennsylvanica', 'ginkgo_biloba', 'gleditsia_triacanthos', 'gymnocladus_dioicus', 'halesia_tetraptera', 'ilex_opaca', 'juglans_cinerea', 'juglans_nigra', 'koelreuteria_paniculata', 'liquidambar_styraciflua', 'liriodendron_tulipifera', 'maclura_pomifera', 'magnolia_acuminata', 'magnolia_denudata', 'magnolia_grandiflora', 'magnolia_macrophylla', 'magnolia_soulangiana', 'magnolia_stellata', 'magnolia_tripetala', 'magnolia_virginiana', 'malus_angustifolia', 'malus_baccata', 'malus_coronaria', 'malus_floribunda', 'malus_hupehensis', 'malus_pumila', 'metasequoia_glyptostroboides', 'morus_alba', 'morus_rubra', 'nyssa_sylvatica', 'ostrya_virginiana', 'oxydendrum_arboreum', 'paulownia_tomentosa', 'phellodendron_amurense', 'pinus_bungeana', 'pinus_cembra', 'pinus_densiflora', 'pinus_echinata', 'pinus_flexilis', 'pinus_koraiensis', 'pinus_nigra', 'pinus_parviflora', 'pinus_peucea', 'pinus_pungens', 'pinus_resinosa', 'pinus_rigida', 'pinus_strobus', 'pinus_sylvestris', 'pinus_taeda', 'pinus_thunbergii', 'pinus_virginiana', 'pinus_wallichiana', 'platanus_acerifolia', 'platanus_occidentalis', 'populus_deltoides', 'populus_grandidentata', 'populus_tremuloides', 'prunus_pensylvanica', 'prunus_sargentii', 'prunus_serotina', 'prunus_serrulata', 'prunus_subhirtella', 'prunus_virginiana', 'prunus_yedoensis', 'ptelea_trifoliata', 'pyrus_calleryana', 'quercus_acutissima', 'quercus_alba', 'quercus_bicolor', 'quercus_cerris', 'quercus_coccinea', 'quercus_falcata', 'quercus_imbricaria', 'quercus_macrocarpa', 'quercus_marilandica', 'quercus_michauxii', 'quercus_montana', 'quercus_muehlenbergii', 'quercus_nigra', 'quercus_palustris', 'quercus_phellos', 'quercus_robur', 'quercus_rubra', 'quercus_shumardii', 'quercus_stellata', 'quercus_velutina', 'quercus_virginiana', 'robinia_pseudo-acacia', 'salix_babylonica', 'salix_caroliniana', 'salix_matsudana', 'salix_nigra', 'sassafras_albidum', 'staphylea_trifolia', 'stewartia_pseudocamellia', 'styrax_japonica', 'styrax_obassia', 'syringa_reticulata', 'taxodium_distichum', 'tilia_americana', 'tilia_cordata', 'tilia_europaea', 'tilia_tomentosa', 'toona_sinensis', 'ulmus_americana', 'ulmus_glabra', 'ulmus_parvifolia', 'ulmus_pumila', 'ulmus_rubra', 'zelkova_serrata']\n",
            "172\n",
            "tensor([ 60,  91,  45, 109, 164, 171, 121, 104, 117, 113,  94, 138, 126,  91,\n",
            "         73,  73, 145, 160,  18,  36, 115, 101,  83, 163,  39, 170, 107, 134,\n",
            "          4, 152, 131,  53])\n",
            "Transform: original, model: vgg16\n",
            "<function vgg16 at 0x7fe51f8d9ef0>\n",
            "Model: VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=172, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----------\t----------\t----------\n",
            "|          \t          \t          |\n",
            "|          \tTRAINING\t          |\n",
            "|          \t          \t          |\n",
            "----------\t----------\t----------\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Epoch 0, validation, Loss: 5.0303 Acc: 0.0208 Time: 240.59332585334778'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epochs:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'||====>                                                                 ||    2/30'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Images processed:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'||===================================>                                  ||    10305/20148'"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg0K5B3m6odx"
      },
      "source": [
        ""
      ]
    }
  ]
}